# Agentes de IA y RAG Visual
## 1. Architecture
## 2. Folder Structure
## Running components independently

The project should be run with Docker Compose, however, if for some reason you want to run every component independently you can do it like so:

### Orchestrator - FastAPI
1. On the /services/api/ folder activate the environment
```
 source /services/api/.venv/bin/activate
```
```
python3 -m main
```
The orchestrator server should start on port 8000. Port defined in code in the uvicorn sentence for FastAPI.

````
INFO:     Started server process [14771]
INFO:     Waiting for application startup.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)
INFO:     127.0.0.1:64541 - "GET /docs HTTP/1.1" 200 OK
INFO:     127.0.0.1:64541 - "GET /openapi.json HTTP/1.1" 200 OK
INFO:     127.0.0.1:64551 - "GET / HTTP/1.1" 200 OK
INFO:     127.0.0.1:64551 - "GET /favicon.ico HTTP/1.1" 404 Not Found
INFO:     127.0.0.1:64558 - "GET /health HTTP/1.1" 200 
````

### Embedding component

1. On the /services/ma
```
 source /services/embedding/.venv/bin/activate
```
```
python3 -m main
```
The orchestrator server should start on port 8001. Port defined in code in the uvicorn sentence for FastAPI.

````
INFO:     Started server process [22432]
INFO:     Waiting for application startup.
Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.
INFO:     Application startup complete.
INFO:     Uvicorn running on http://0.0.0.0:8001 (Press CTRL+C to quit)

````

#### Here  the warning means
The CLIP model (or its processor)  was originally saved with a slow image processor.

In future versions of Hugging Face Transformers (starting with 4.52), even if the model was saved with a slow processor, the default will be to use the fast processor (use_fast=True).

This might cause small differences in the outputs, but nothing critical.

If you still want to use the slow processor, you’ll have to explicitly set:

````
CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32", use_fast=False)
````

Using a slow processor for CLIP (or any Hugging Face processor/tokenizer) mainly has performance implications, not functional ones.

Here’s the breakdown:

1. **Performance impact**
Slow processor: Written in pure Python — generally slower at handling image preprocessing (resizing, normalization, batching).

Fast processor: Uses Rust-based tokenizers / optimized image libraries — faster and more memory efficient, especially on large datasets.

For example:

Processor type	1k images	100k images
Slow	~1.5 min	~2.5 hours
Fast	~20 sec	~30 min

2. **Output differences**
The preprocessing logic should be almost identical.

You might see minor numeric differences (e.g., float rounding) because of how libraries handle resizing and normalization internally.

These differences are usually insignificant for retrieval tasks like your Visual RAG.

3. **Compatibility going forward**
Hugging Face plans to make use_fast=True the default.

For the sake of exploration we can go ahead and use_fast=False

### Test

This command generates the embedding of an image

```

`curl -X POST "http://localhost:8002/embed"   -F "file=@/path/to/image/*.png" | jq .`
```

The result should be a json file with the embeddings generated by the image, with the following format:
```
{
  "dim": 512,
  "vector": [
   
  ]
}
````




## Running components with Docker Compose